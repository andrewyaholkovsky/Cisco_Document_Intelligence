{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b43fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ~/.zshrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f545cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    " \n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07abb9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pdf2image\n",
    "#pip install pytesseract\n",
    "#pip install langchain\n",
    "#pip install openai\n",
    "\n",
    "from langchain import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import LLMChain\n",
    "\n",
    "\n",
    "llm = OpenAI(openai_api_key=openai_api_key, temperature=0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "131fe0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f762ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "def convert_pdf_to_images(pdf_path):\n",
    "    return convert_from_path(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "824076ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "def extract_text_from_images(images):\n",
    "    extracted_text = ''\n",
    "    for image in images:\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        extracted_text += text\n",
    "    return extracted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b01c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = 'your-api-key'\n",
    "\n",
    "def query_openai_with_context(cleaned_text):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-004\",  # Or the latest model\n",
    "        prompt=cleaned_text,\n",
    "        max_tokens=150  # Adjust as needed\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8407f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Vivi7/gs-12-1565.pdf\"\n",
    "image = convert_pdf_to_images(path)\n",
    "text = extract_text_from_images(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80ef348b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NUMBER TYPE\\nGENERAL\\nGS-12-1565 PRODUCT SPECIFICATION Amphenol ICC\\nTITLE PAGE REVISION\\n1 of 12 H\\nEXAM'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "874c2dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7033"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6d380ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Supplier Name': 'Amphenol ICC', 'Temperature': '-55 to 85 °C'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_extraction_chain\n",
    "\n",
    "# Schema\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"Supplier Name\": {\"type\": \"string\"},\n",
    "        \"Temperature\": {\"type\": \"string\"},\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Run chain\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\", openai_api_key=openai_api_key)\n",
    "chain = create_extraction_chain(schema, llm)\n",
    "chain.run(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1085ca8",
   "metadata": {},
   "source": [
    "# Schema and \"Extra Info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4393bae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Product Type': 'Connector System',\n",
       " 'Supplier Name': 'Amphenol ICC',\n",
       " 'Product Name': 'EXAMAX2™ and EXAMEZZ2™',\n",
       " 'Dimensions': '',\n",
       " 'Orientation': '',\n",
       " 'Voltage': '< 30 VAC RMS / DC',\n",
       " 'Frequency': '',\n",
       " 'Impedence/Capacitance': '',\n",
       " 'Temperature': '-55 to 85 °C',\n",
       " 'Temperature_extra_info': ''}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"Product Type\": {\"type\": \"string\"},\n",
    "        \"Supplier Name\": {\"type\": \"string\"},\n",
    "        \"Product Name\": {\"type\": \"string\"},\n",
    "        \"Dimensions\": {\"type\": \"string\"},\n",
    "        \"Orientation\": {\"type\": \"string\"},\n",
    "        \"Voltage\": {\"type\": \"string\"},\n",
    "        \"Frequency\": {\"type\": \"string\"},\n",
    "        \"Impedence/Capacitance\": {\"type\": \"string\"},\n",
    "        \"Temperature\": {\"type\": \"string\"},\n",
    "        \"Temperature_extra_info\": {\"type\": \"string\"}\n",
    "\n",
    "    },\n",
    "}\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-16k\",openai_api_key=openai_api_key)\n",
    "chain = create_extraction_chain(schema, llm)\n",
    "chain.run(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2d2bcfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_other = \"Vivi7/UPY_GPHC_Y5V_6_3V_to_50V_11-3005518.pdf\"\n",
    "image_other = convert_pdf_to_images(path_other)\n",
    "text_other = extract_text_from_images(image_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8f35b425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Product Type': 'Surface-Mount Ceramic Multilayer Capacitors',\n",
       " 'Supplier Name': 'YAGEO',\n",
       " 'Product Name': 'YSV',\n",
       " 'Dimensions': '0201, 0402, 0603, 0805, 1206, 1210',\n",
       " 'Orientation': '',\n",
       " 'Voltage': '6.3 V to 50 V',\n",
       " 'Frequency': '',\n",
       " 'Impedence/Capacitance': '10 nF to 22 uF',\n",
       " 'Temperature': '-30 °C to +85 °C',\n",
       " 'Temperature_extra_info': 'Y5V: -30 °C to +85 °C'}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(text_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d03b4cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Product Type': 'Enhanced High Power Card Edge',\n",
       " 'Supplier Name': 'Amphenol ICC',\n",
       " 'Product Name': 'eHPCE® connector system',\n",
       " 'Dimensions': '',\n",
       " 'Orientation': 'Right Angle',\n",
       " 'Voltage': '300Vpc for power contact, 100Vpc for signal contact',\n",
       " 'Frequency': '',\n",
       " 'Impedence/Capacitance': '',\n",
       " 'Temperature': '-55°C ~ +140°C',\n",
       " 'Temperature_extra_info': 'includes the terminal temperature rise when powered'}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path2 = \"Vivi7/gs-12-1457.pdf\"\n",
    "image2 = convert_pdf_to_images(path2)\n",
    "text2 = extract_text_from_images(image2)\n",
    "chain.run(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f2091bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Product Type': 'Mini Cool Edge 0.60mm Connectors',\n",
       " 'Supplier Name': 'Amphenol',\n",
       " 'Product Name': 'Mini Cool Edge',\n",
       " 'Dimensions': '0.60mm',\n",
       " 'Orientation': 'Vertical, right angle, straddle mount and orthogonal',\n",
       " 'Voltage': '',\n",
       " 'Frequency': '32GT/s to 56GT/s',\n",
       " 'Impedence/Capacitance': '',\n",
       " 'Temperature': '',\n",
       " 'Temperature_extra_info': ''}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path3 = \"Vivi7/ssio_mini_cooledge_0_60mm.pdf\"\n",
    "image3 = convert_pdf_to_images(path3)\n",
    "text3 = extract_text_from_images(image3)\n",
    "chain.run(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6ca2b5cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse function call data: Expecting property name enclosed in double quotes: line 13 column 3 (char 380)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/output_parsers/openai_functions.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     97\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                         return json.loads(\n\u001b[0m\u001b[1;32m     99\u001b[0m                             \u001b[0mfunction_call\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"arguments\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mkw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parse_constant'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 13 column 3 (char 380)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xg/8zx_jm0x3bddtdbddx13kckm0000gn/T/ipykernel_32103/137309491.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimage4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_pdf_to_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text_from_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`run` supports only one positional argument.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             return self(args[0], callbacks=callbacks, tags=tags, metadata=metadata)[\n\u001b[0m\u001b[1;32m    506\u001b[0m                 \u001b[0m_output_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             ]\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         final_outputs: Dict[str, Any] = self.prep_outputs(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m             outputs = (\n\u001b[0;32m--> 304\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    107\u001b[0m     ) -> Dict[str, str]:\n\u001b[1;32m    108\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     def generate(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mcreate_outputs\u001b[0;34m(self, llm_result)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_result\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLLMResult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;34m\"\"\"Create outputs from response.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         result = [\n\u001b[0m\u001b[1;32m    264\u001b[0m             \u001b[0;31m# Get the text of the top generated string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             {\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;31m# Get the text of the top generated string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             {\n\u001b[0;32m--> 266\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0;34m\"full_generation\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             }\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/output_parsers/openai_functions.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGeneration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpartial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/langchain/output_parsers/openai_functions.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    100\u001b[0m                         )\n\u001b[1;32m    101\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                         raise OutputParserException(\n\u001b[0m\u001b[1;32m    103\u001b[0m                             \u001b[0;34mf\"Could not parse function call data: {exc}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                         )\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse function call data: Expecting property name enclosed in double quotes: line 13 column 3 (char 380)"
     ]
    }
   ],
   "source": [
    "path4 = \"Vivi7/2031430001-PS.pdf\"\n",
    "image4 = convert_pdf_to_images(path4)\n",
    "text4 = extract_text_from_images(image4)\n",
    "chain.run(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e9c18a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Product Type': 'Aluminum Electrolytic Capacitor',\n",
       " 'Supplier Name': 'LELON ELECTRONICS CORP.',\n",
       " 'Product Name': 'VZH680M2ATR-1313S',\n",
       " 'Dimensions': '12.5mm x 13.5mm',\n",
       " 'Orientation': 'Vertical',\n",
       " 'Voltage': '100 V',\n",
       " 'Frequency': '120 Hz',\n",
       " 'Impedence/Capacitance': '68 uF',\n",
       " 'Temperature': '-55°C ~ +105°C',\n",
       " 'Temperature_extra_info': 'Capacitance Tolerance -20% ~ +20% (120 Hz, 20°C), Surge Voltage 125 VDC, Leakage Current < 68yA After 2 minutes, Dissipation Factor (Tand) < 0.07 (120 Hz, 20°C), Impedance < 0.320 (100kHz, 20°C), Ripple Current (rms) 450 mA (100kHz, 105°C), Low Temperature Z(-25°C /Z(+20°C ) 2, Characteristics(120 Hz) Z(-55°C /Z(+20°C ) 3, Ripple Current & Frequency (Hz)} 50, 60 120 1k 10k up, Frequency Multipliers Multiplier 0.60 0.70 0.85 1.00, Endurance: Capacitance Change Within +30 % of initial value After 5000 Hrs at 105°C, Dissipation factor Less than 300% of specified value After 1000 Hrs at 105°C, Leakage Current Within specified value After 1000 Hrs at 105°C, Standards JIS C 5101-1, -18, Remarks RoHS Compliance & Halogen-free'}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path5 = \"Vivi7/vzh680m2atr-1313s.pdf\"\n",
    "image5 = convert_pdf_to_images(path5)\n",
    "text5 = extract_text_from_images(image5)\n",
    "chain.run(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "29f20244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Product Type': 'Capacitor',\n",
       " 'Supplier Name': 'KYOCERA',\n",
       " 'Product Name': '',\n",
       " 'Dimensions': '1608(JIS) / 0603(EIA) L ~ w',\n",
       " 'Orientation': '',\n",
       " 'Voltage': '25Vdc',\n",
       " 'Frequency': '10, 20, 0, 0, 1, -20, -40, -60, -80, -100, 0.01, 0.1, 1, 10, 100, 1000',\n",
       " 'Impedence/Capacitance': '10uF',\n",
       " 'Temperature': '-55deg to85deg',\n",
       " 'Temperature_extra_info': 'Temp. coeff +/-15%, -60, -80, -100, 0.001, -100'}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path6 = \"Vivi7/KGM15CR51E106K-DATA.pdf\"\n",
    "image6 = convert_pdf_to_images(path6)\n",
    "text6 = extract_text_from_images(image6)\n",
    "chain.run(text6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb31092",
   "metadata": {},
   "source": [
    "# Single Attribute Extraction (no extraction chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "343efc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Product Type\": \"EXAMAX2™ and EXAMEZZ2™ Connector System\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#from langchain.llms import OpenAIChat\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo-16k',\n",
    "    temperature = 0,\n",
    "    openai_api_key = openai_api_key      \n",
    "    #max_tokens=self.config.llm.max_tokens                 \n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"I want to find the following attribute Product Type from the document below. If an attribute cannot be found, return an empty string. Return the results in JSON format. Document: {doc_text}\"\"\"\n",
    "\n",
    "#llm = OpenAI(temperature=0,model=\"gpt-3.5-turbo-16k\", openai_api_key=\"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\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "response = llm_chain(text)\n",
    "json_response = response.get('text')\n",
    "print(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "56cd1dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Supplier Name\": \"Amphenol ICC\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#from langchain.llms import OpenAIChat\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo-16k',\n",
    "    temperature = 0,\n",
    "    openai_api_key = openai_api_key     \n",
    "    #max_tokens=self.config.llm.max_tokens                 \n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"I want to find the following attribute Supplier Name from the document below. If an attribute cannot be found, return an empty string. Return the results in JSON format. Document: {doc_text}\"\"\"\n",
    "\n",
    "#llm = OpenAI(temperature=0,model=\"gpt-3.5-turbo-16k\", openai_api_key=\"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\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "response = llm_chain(text)\n",
    "json_response = response.get('text')\n",
    "print(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f1fe8637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Supplier Name\": \"Amphenol ICC\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#from langchain.llms import OpenAIChat\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo-16k',\n",
    "    temperature = 0,\n",
    "    openai_api_key = openai_api_key    \n",
    "    #max_tokens=self.config.llm.max_tokens                 \n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"I want to find the following attribute Supplier Name from the document below. Return the results in JSON format. If an attribute cannot be found, return an empty string. Document: {doc_text}\"\"\"\n",
    "\n",
    "#llm = OpenAI(temperature=0,model=\"gpt-3.5-turbo-16k\", openai_api_key=\"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\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "response = llm_chain(text)\n",
    "json_response = response.get('text')\n",
    "print(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5db48e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Dimensions\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#from langchain.llms import OpenAIChat\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo-16k',\n",
    "    temperature = 0,\n",
    "    openai_api_key = openai_api_key      \n",
    "    #max_tokens=self.config.llm.max_tokens                 \n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"I want to find the following attribute Dimensions from the document below. If an attribute cannot be found, return an empty string. Return the results in JSON format. Document: {doc_text}\"\"\"\n",
    "\n",
    "#llm = OpenAI(temperature=0,model=\"gpt-3.5-turbo-16k\", openai_api_key=\"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\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "response = llm_chain(text)\n",
    "json_response = response.get('text')\n",
    "print(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6a0ca8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Orientation\": \"Right Angle\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#from langchain.llms import OpenAIChat\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo-16k',\n",
    "    temperature = 0,\n",
    "    openai_api_key = openai_api_key      \n",
    "    #max_tokens=self.config.llm.max_tokens                 \n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"I want to find the following attribute Orientation from the document below. If an attribute cannot be found, return an empty string. Return the results in JSON format. Document: {doc_text}\"\"\"\n",
    "\n",
    "#llm = OpenAI(temperature=0,model=\"gpt-3.5-turbo-16k\", openai_api_key=\"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\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "response = llm_chain(text)\n",
    "json_response = response.get('text')\n",
    "print(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8a825dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Voltage\": \"< 30 VAC RMS / DC\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#from langchain.llms import OpenAIChat\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo-16k',\n",
    "    temperature = 0,\n",
    "    openai_api_key = openai_api_key      \n",
    "    #max_tokens=self.config.llm.max_tokens                 \n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"I want to find the following attribute Voltage from the document below. If an attribute cannot be found, return an empty string. Return the results in JSON format. Document: {doc_text}\"\"\"\n",
    "\n",
    "#llm = OpenAI(temperature=0,model=\"gpt-3.5-turbo-16k\", openai_api_key=\"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\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "response = llm_chain(text)\n",
    "json_response = response.get('text')\n",
    "print(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5d275aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Impedence/Capacitance\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#from langchain.llms import OpenAIChat\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo-16k',\n",
    "    temperature = 0,\n",
    "    openai_api_key = openai_api_key      \n",
    "    #max_tokens=self.config.llm.max_tokens                 \n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"I want to find the following attribute Impedence/Capacitance from the document below. If an attribute cannot be found, return an empty string. Return the results in JSON format. Document: {doc_text}\"\"\"\n",
    "\n",
    "#llm = OpenAI(temperature=0,model=\"gpt-3.5-turbo-16k\", openai_api_key=\"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\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "response = llm_chain(text)\n",
    "json_response = response.get('text')\n",
    "print(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "22d521c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Temperature\": \"-55 to 85 °C\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#from langchain.llms import OpenAIChat\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo-16k',\n",
    "    temperature = 0,\n",
    "    openai_api_key = openai_api_key      \n",
    "    #max_tokens=self.config.llm.max_tokens                 \n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"I want to find the following attribute Temperature from the document below. If an attribute cannot be found, return an empty string. Return the results in JSON format. Document: {doc_text}\"\"\"\n",
    "\n",
    "#llm = OpenAI(temperature=0,model=\"gpt-3.5-turbo-16k\", openai_api_key=\"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\")\n",
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "\n",
    "response = llm_chain(text)\n",
    "json_response = response.get('text')\n",
    "print(json_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61132fe6",
   "metadata": {},
   "source": [
    "# Checking Token Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9d2abd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "77934da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 7145\n",
      "\tPrompt Tokens: 7058\n",
      "\tCompletion Tokens: 87\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.021522\n"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    llm_chain(text)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c3c72a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "61f2202d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7033"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "413fcb90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.get_num_tokens(response.get('text'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f41dc29",
   "metadata": {},
   "source": [
    "# Manual (not LangChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ec337b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "\n",
    "# Set this once for the duration of the notebook\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "def ask_question_about_document_16k(document, question):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": document},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0785f352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The main topic of the document is the product specification for the EXAMAX2™ and EXAMEZZ2™ Connector System by Amphenol ICC.\n"
     ]
    }
   ],
   "source": [
    "# Example use:\n",
    "question = \"What is the main topic of the document?\"\n",
    "answer = ask_question_about_document_16k(text, question)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fc363525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# Load your API key from an environment variable or secret management service\n",
    "#openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9da8f6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-8Hx7Oc4ORc3GBsqsTiKuyLjQbyof3 at 0x7f77292a0180> JSON: {\n",
       "  \"id\": \"chatcmpl-8Hx7Oc4ORc3GBsqsTiKuyLjQbyof3\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1699289518,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"Hello! How can I assist you today?\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 9,\n",
       "    \"completion_tokens\": 9,\n",
       "    \"total_tokens\": 18\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d0a543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
